#### HuggingFace：一键部署自己的大模型

#### Transformers Pipeline

#### Pipeline 的基本功能

把 Runtime 的类型修改为 GPU。

```
from transformers import pipeline
classifier = pipeline(task="sentiment-analysis", device=0)
preds = classifier("I am really happy today!")
print(preds)
```

输出结果：

```
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revi
Using a pipeline without specifying a model name and revision in production is not recommend
[{'label': 'POSITIVE', 'score': 0.9998762607574463}]
```

这个代码非常简单，第一行代码，我们定义了一个 task 是 sentimental-analysis 的 Pipeline，也就是一个情感分析的分类器。里面 device=0 的意思是我们指定让 Transformer 使用 GPU 资源。如果你想要让它使用 CPU，你可以设置 device=-1。然后，调用这个分类器对一段文本进行情感分析。从输出结果看，它给出了正确的 Positive 预测，也给出了具体的预测分数。因为我们在这里没有指定任何模型，所以 Transformers 自动选择了默认的模型，也就是日志里看到的 distilbert-base-uncased-finetuned-sst-2-english 这个模型。

换成针对中文的模型：

```
classifier = pipeline(model="uer/roberta-base-finetuned-jd-binary-chinese", task="sentiment-analysis", device=0)
preds = classifier("这个餐馆太难吃了。")
print(preds)
```

输出结果：

```
[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.934112012386322}]
```

roberta-base-finetuned-jd-binary-chinese（RoBERTa）这个模型是基于 BERT 做了一些设计上的修改而得来的。而后面的 finetuned-jd-binary-chinese 是基于京东的数据进行微调过的一个模型。

Pipeline 是 Transformers 库里面的一个核心功能，它封装了所有托管在 HuggingFace 上的模型推理预测的入口。如果业务场景更欢乐，只需要把 task 换成 translation_en_to_zh，然后选用一个合适的模型就好了。

```
translation = pipeline(task="translation_en_to_zh", model="Helsinki-NLP/opus-mt-en-zh", device=0)

text = "I like to learn data science and AI."
translated_text = translation(text)
print(translated_text)
```

输出结果：

```
[{'translation_text': '我喜欢学习数据科学和人工智能'}]
```

#### 如何寻找自己需要的模型？

这个时候，我们就需要去 HuggingFace 的网站里找一找了。你点击网站的 [Models 板块](https://huggingface.co/models)，就可以看到一个界面，左侧是一系列的筛选器，而右侧则是筛选出来的模型。

我们点击 Helsinki-NLP/opus-mt-en-zh 进入这个模型的卡片页，就能看到更详细的介绍。并且很多模型，都在右侧提供了对应的示例。不使用代码，你也可以直接体验一下模型的能力和效果。

#### Pipeline 支持的自然语言处理任务

Transformers 的 Pipeline 模块，支持的 task 是非常丰富的。可以说大部分常见的自然语言处理
任务都被囊括在内了，经常会用到的有这么几个。

1. feature-extraction，其实它和 OpenAI 的 Embedding 差不多，也就是把文本变成一段向量。
2. fill-mask，也就是完形填空。你可以把一句话中的一部分遮盖掉，然后让模型预测遮盖掉的地方的词是什么。
3. ner，命名实体识别。我们常常用它来提取文本里面的时间、地点、人名、邮箱、电话号码、地址等信息，然后进一步用这些信息来处理其他任务。
4. question-answering 和 table-question-answering，专门针对问题进行自动问答，在客服的 FAQ 领域常常会用到这类任务。
5. sentiment-analysis 和 text-classification，也就是我们之前见过的情感分析，以及类目更自由的文本分类问题。
6. text-generation 和 text2text-generation，文本生成类型的任务。我们之前让 AI 写代码或者写故事，其实都是这一类的任务。

剩下还有 summarization 文本摘要、translation 机器翻译、以及 zero-shot-classification，也就是开课一开始讲的零成本分类。

#### 通过 Pipeline 进行语音识别

Pipeline 不仅支持自然语言处理相关的任务，它还支持语音和视觉类的任务。比如，我们同样可以通过 Pipeline 使用 OpenAI 的 Whisper 模型来做语音识别。

```
from transformers import pipeline
transcriber = pipeline(model="openai/whisper-medium", device=0)
result = transcriber("./data/podcast_clip.mp3")
print(result)
```

输出结果：

```
{'text': " Welcome to OnBoard, a real first-line experience, a new investment thinking. I'm..."}
```

在实际调用 Whisper 模型的时候，它会在最终生成文本的过程里面，加入一个<|en|>，导致文本生成的时候强行被指定成了英文。我们可以修改一下这个 decoder 生成文本时的设置，让输出的内容变成中文。

#### 如何使用 Inference API？

#### 尝试 Inference API

首先，和其他的 API Key 一样，我们还是通过环境变量来设置一下 Huggingface 的 AccessToken。你可以在 Huggingface 的个人设置里面拿到这个 Key，然后通过 export 设置到环境变量里就好了。

设置环境变量：

```
export HUGGINGFACE_API_KEY=YOUR_HUGGINGFACE_ACCESS_TOKEN
```

#### 等待模型加载完毕

同样的，Inference API 也支持各种各样的任务。我们在模型页的卡片里，如果能够看到一个带着闪电标记 ⚡ 的 Hosted Inference API 字样，就代表着这个模型可以通过 Inference API 调用。并且下面可以让你测试的示例，就是这个 Inference API 支持的任务。

hfl/chinese-pert-base 模型支持的就是 feature-extraction 的任务，它能够让你把自己的文本变成向量。

```
model = "hfl/chinese-pert-base"
API_URL = f"https://api-inference.huggingface.co/models/{model}"
question = "今天天气真不错！"
data = query({"inputs" : question}, api_url=API_URL)
print(data)
```

输出结果：

```
{'error': 'Model hfl/chinese-pert-base is currently loading', 'estimated_time': 20.0}
```

#### 如何部署自己的大模型?

HuggingFace 自己就提供了一个非常简便的部署开源模型的产品，叫做 Inference Endpoint。你不需要自己去云平台申请服务器，搭建各种环境。只需要选择想要部署的模型、使用的服务器资源，一键就能把自己需要的模型部署到云平台上。

#### 把模型部署到 Endpoint 上

其实 GPT2 的论文里，已经体现了大语言模型不少潜力了。那么，下面我们就试着来部署一下 GPT2 这个模型。

1. 首先，进入创建 Endpoint 的界面，你可以选择自己想要部署的模型，我们这里选择了
   GPT2 这个模型。
2. Endpoint Name，你可以自己设置一个，也可以直接使用系统自动生成的。
3. 系统默认会为你选择云服务商、对应的区域，以及需要的硬件资源。如果你选择的硬件资
   源不足以部署这个模型，页面上也会有对应的提示告诉你。GPT2 的模型连 GPU 也不需要，
   有 CPU 就能运行起来。
4. 最后你需要选择一下这个 Endpoint 的安全等级，一共有三种，分别是 Protected、Public
   和 Private。

设置好了之后，你再点击最下面的 Create Endpoint，HuggingFace 就会开始帮你创建机器资源，部署对应的模型了。

#### 测试体验一下大模型

部署完成之后，我们会自动进入对应的 Endpoint 详情页里。上面的 Endpoint URL 就表示你可以像调用 Inference API 一样调用模型的 API_URL。而下面，也给出了一个测试输入框，这个测试输入框我们在 HuggingFace 模型卡片页面里也能够看到。

#### 暂停、恢复以及删除 Endpoint

部署在 Endpoint 上的模型是按照在线的时长收费的。如果你暂时不用这个模型，可以选择暂停（Pause）这个 Endpoint。等到想使用的时候，再重新恢复（Resume）这个 Endpoint 就好了。暂停期间的模型不会计费，这个功能的选项就在模型 Overview 标签页的右上角。

如果你彻底不需要使用这个模型了，你可以把对应的 Endpoint 删掉，你只需要在对应 Endpoint 的 Setting 页面里输入 Endpoint 的名称，然后选择删除就好了。

HuggingFace 将部署一个开源模型到线上的成本基本降低到了 0。不过，目前它只支持海外的 AWS、Azure 以及 Google Cloud，并不支持阿里云或者腾讯云，对国内的用户算是一个小小的遗憾。
