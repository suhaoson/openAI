[06](./06.md) 讲实现一个聊天机器人为了能够让 ChatGPT 知道整个聊天的上下文，我们需要把历史的对话记录都传给它。但是，因为能够接收的 Token 数量有上限，所以我们只能设定一个参数，只保留最后几轮对话。我们最后把这个功能，抽象成了一个 Conversation 类。

#### BufferWindow，滑动窗口记忆

在 LangChain 里，把对于整个对话过程的上下文叫做 Memory。任何一个 LLMChain，我们都可以给它加上一个 Memory。

可以看到，我们做的事情其实和之前的 Conversation 类似，我们定义了一个 PromptTemplate 来输入我们的指示。然后，在 LLMChain 构造的时候，我们为它指定了一个叫做 ConversationBufferWindowMemory 的 memory 对象，并且为这个 memory 对象定义了 k=3，也就是只保留最近三轮的对话内容。

连续跟他对话，第五轮的时候它就已经变成“鱼香肉丝怎么做？”了。可以使用 memory 的 load_memory_variables 方法，它会直接返回 memory 里实际记住的对话内容。

#### SummaryMemory，把小结作为历史记忆

BufferWindow 会在几轮对话后，就把一开始聊的内容给忘了。[07](./07.md) 讲的时候，我们可以对前面的几轮对话做个总结。

其中代码有两个需要注意的点：

第一个是对于我们定义的 ConversationSummaryMemory，它的构造函数也接受一个 LLM 对象。这个对象会专门用来生成历史对话的小结，是可以和对话本身使用的 LLM 对象不同的。

第二个是这次我们没有使用 LLMChain 这个对象，而是用了封装好的 ConversationChain。用 ConversationChain 的话，其实我们是可以不用自己定义 PromptTemplate 来维护历史聊天记录的，但是为了使用中文的 PromptTemplate，我们在这里还是自定义了对应的 Prompt。

在我们打开了 ConversationChain 的 Verbose 模式，然后再次询问 AI 第二个问题的时候，你可以看到，在 Verbose 的信息里面，没有历史聊天记录，而是多了一段对之前聊天内容的英文小结。

而如果这个时候我们调用 memory 的 load_memory_variables 方法，可以看到记录下来的 history 是一小段关于对话的英文小结。而不是像上面那样，记录完整的历史对话。

#### 两者结合，使用 SummaryBufferMemory

虽然 SummaryMemory 可以支持更长的对话轮数，但是它也有一个缺点，就是即使是最近几轮的对话，记录的也不是精确的内容。可以用 BufferMemory 和 SummaryMemory 结合一下就可以了，LangChain 提供了一个类似这样的解决方案，ConversationSummaryBufferMemory。

我们定义了一个 ConversationSummaryBufferMemory，在这个 Memory 的构造函数里面，我们指定了使用的 LLM、提示语，以及一个 max_token_limit 参数。max_token_limit 参数，其实就是告诉我们，当对话的长度到多长之后，我们就应该调用 LLM 去把文本内容小结一下。

运行程序有点忙，因为 使用 ConversationSummaryBufferMemory 很多时候需要多次调用 OpenAI 的 API。在字数超过 max_token_limit 的时候，需要额外调用一次 API 来做小结。而且这样做，对应的 Token 数量消耗也是不少的。

不是所有的任务，都适合通过调用一次 ChatGPT 的 API 来解决。很多时候，你还是可以多思考是否可以用上一讲介绍的 UtilityChain 和 TransformChain 来解决问题。

#### 让 AI 记住点有用的信息

上边的通过调用 memory.load_memory_variables 方法，我们发现 AI 对整段对话做了小结。但是这个小结有个问题，就是它并没有提取到我们最关注的信息。

Langchain 也内置了一个 EntityMemory 的封装，让 AI 自动帮我们提取这样的信息。
