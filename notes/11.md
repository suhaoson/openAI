开源模型：

#### 在 Colab 里使用 GPU

1. 你先选择菜单栏里的 Runtime，然后点击 Change runtime type。
2. 然后在弹出的对话框里，把 Hardware accelerator 换成 GPU，然后点击 Save 就可以了。

#### HuggingfaceEmbedding，你的开源伙伴

llama-index 支持你自己直接定义一个定制化的 Embedding。使用 llama-index 向量搜索部分用开源模型的 Embedding 替换掉。

```
conda install -c conda-forge sentence-transformers
```

```
import openai, os
import faiss
from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTFaissIndex, ServiceContext
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from llama_index.node_parser import SimpleNodeParser

openai.api_key = ""

text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=20)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./data/faq/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes,faiss_index=faiss_index, service_context=service_context)
```

输出结果：

```
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu
WARNING:root:Created a chunk of size 130, which is longer than the specified 100
……
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total LLM token usage: 0 tokens
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] Total embedding token usage: 3198 tokens
```

这个例子使用了一个面向电商的 FAQ 的纯文本文件作为输入。预设了一些问答对。

首先 openai.api_key 设置为空字符串

定义一个 embeded_model，这个 embeded_model 包装的是一个 HuggingFaceEmbeddings 的类。

因为 HuggingFace 为基于 transformers 的模型定义了一个标准，只要传入一个模版名称，这个类就会下载模版、加载模版，并通过模型来计算你输入的文本 Embedding。使用 HuggingFace 的好处是，你可以通过一套代码使用所有的 transfomers 类型的模型。

sentence-transformers 是目前效果最好的语义搜索类的模型，它在 BERT 的基础上采用了<b>对比学习的方式</b>，来区分文本语义的相似度，它包括了一系列的预训练模型。

我们还是使用 Faiss 这个库来作为我们的向量索引库，所以需要指定一下向量的维度，paraphrase-multilingual-mpnet-base-v2 这个模型的维度是 768，所以我们就把维度定义成 768 维。

相应的对文档的切分，我们使用的是 CharacterTextSplitter，并且在参数上我们做了一些调整。

1. 首先，我们把“\n\n”这样两个连续的换行符作为一段段文本的分隔符，因为我们的 FAQ 数据里，每一个问答对都有一个空行隔开，正好是连续两个换行。
2. 然后，我们把 chunk_size 设置得比较小，只有 100。因为我们所使用开源模型是个小模型，这样我们才能在单机加载起来。如果我们不设置 chunk_size，llama-index 会自动合并多个 chunk 变成一个段落。
3. 其次，我们还增加了一个小小的参数，叫做 chunk_overlap。这个参数代表我们自动合并小的文本片段的时候，可以接受多大程度的重叠。默认值是 200，超过了单段文档的 chunk_size，所以我们这里要把它设小一点，不然程序会报错。

我们可以在对应的 verbose 日志里看到，这里的 Embedding 使用了 3198 个 Token，不过这些 Token 都是我们通过 sentence_transformers 类型的开源模型计算的，不需要花钱。你的成本就节约下来了。

#### 使用 ChatGLM 提供对话效果

[ChatGLM](https://github.com/THUDM/GLM-130B) 最大的一个模型有 1300 亿个参数。

但是这么大的模型，无论是你自己的电脑，还是 Colab 提供的 GPU 和 TPU 显然都放不了。所以我们只能选用一个裁剪后的 60 亿个参数的版本，并且我们还必须用 int-4 量化的方式，而不是用 float16 的浮点数。--chatglm-6b-int4

```
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
model = model.eval()
```

我们希望通过 GPU 进行模型计算，加载模型的时候调用了.cuda()。

这里加载模型的时候，我们还设置了一个 trust_remote_code = true 的参数，这是因为 ChatGLM 的模型不是一个 Huggingface 官方发布的模型，而是由用户贡献的，所以需要你显式确认你信任这个模型的代码，它不会造成恶意的破坏。我们反正是在 Colab 里面运行这个代码，所以倒是不用太担心。

如果你想要用 CPU 运行，可以把模型加载的代码换成下面这样。（不建议）

```
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).float()
```

#### 将 ChatGLM 封装成 LLM

不过上面的代码里面，我们用的还是原始的 ChatGLM 的模型代码，还不能直接通过 query 来访问 llama-index 直接得到答案。只要我们把它封装成一个 LLM 类，让我们的 index 使用这个指定的大语言模型就好了。对应[llama-index 的文档](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html)。

```
import openai, os
import faiss
from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTFaissIndex, ServiceContext
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from llama_index.node_parser import SimpleNodeParser

from langchain.llms.base import LLM
from llama_index import LLMPredictor
from typing import Optional, List, Mapping, Any

class CustomLLM(LLM):
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response, history = model.chat(tokenizer, prompt, history=[])
        return response

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"name_of_model": "chatglm-6b-int4"}

    @property
    def _llm_type(self) -> str:
        return "custom"
```

重新运行一下我们的问题，看看效果是怎么样的？

```
from langchain.text_splitter import SpacyTextSplitter

llm_predictor = LLMPredictor(llm=CustomLLM())

text_splitter = CharacterTextSplitter(separator="\n\n", chunk_size=100, chunk_overlap=20)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/faq/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes, faiss_index=faiss_index, service_context=service_context)
```

现在，我们有了一个通过 paraphrase-multilingual-mpnet-base-v2 模型来计算 Embeddding 并进行语义搜索，然后通过 chatglm-6b-int4 的模型来进行问答的解决方案了。而且这两个模型，可以跑在一块家用级别的显卡上。

#### 开源模型的不足之处

看样子，能够在本机运行的小模型基本完成了。数据安全、又不用担心花费。但显然刚刚处理电商问答比较简单，我们再拿一个稍微复杂一点的问题来看看效果。

```
text_splitter = SpacyTextSplitter(pipeline="zh_core_web_sm", chunk_size = 128, chunk_overlap=32)
parser = SimpleNodeParser(text_splitter=text_splitter)
documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/zhaohuaxishi/').load_data()
nodes = parser.get_nodes_from_documents(documents)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
))
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)

dimension = 768
faiss_index = faiss.IndexFlatIP(dimension)
index = GPTFaissIndex(nodes=nodes, faiss_index=faiss_index, service_context=service_context)
```

输入的索引数据是鲁迅整套的《朝花夕拾》。

对应的模型和索引加载的代码基本一致，只有一个小小的区别，就是在文本分割的时候，我们用了上一讲介绍过的 SpacyTextSplitter，因为这里都是散文的内容，而不是确定好格式的 QA 对。所以通过 SpacyTextSplitter 来分句，并在允许的时候合并小的片段是有意义的。

```

# query will use the same embed_model
from llama_index import QueryMode
from llama_index import QuestionAnswerPrompt

openai.api_key = os.environ.get("OPENAI_API_KEY")

QA_PROMPT_TMPL = (
    "下面的内容来自鲁迅先生的散文集《朝花夕拾》，很多内容是以第一人称写的 \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "根据这些信息，请回答问题: {query_str}\n"
    "如果您不知道的话，请回答不知道\n"
)
QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)

response = index.query(
    "鲁迅先生在日本学习医学的老师是谁？",
    mode=QueryMode.EMBEDDING,
    similarity_top_k = 1,
    text_qa_template=QA_PROMPT,
    verbose=True,
)
print(response)
```

输出结果：

```
> Got node text: 一将书放在讲台上，便用了缓慢而很有顿挫的声调，向学生介绍自己道：——
    “我就是叫作藤野严九郎的……。”


后面有几个人笑起来了。
他接着便讲述解剖学在日本发达的历史，那些大大小小的书，便是从最初到现今关于这一门学问的著作。...

鲁迅先生在日本学习医学的老师是藤野严九郎。
```

```
response = index.query(
    "鲁迅先生是在日本的哪个城市学习医学的？",
    mode=QueryMode.EMBEDDING,
    similarity_top_k = 1,
    text_qa_template=QA_PROMPT,
    verbose=True,
)
print(response)
```

输出结果：

```
> Got node text: 有时我常常想：他的对于我的热心的希望，不倦的教诲，小而言之，是为中国，就是希望中国有新的医学；大而言之，是为学术，就是希望新的医学传到中国去。...

根据这些信息，无法得出鲁迅先生是在日本的哪个城市学习医学的答案。
```

可以看到，有些问题在这个模式下，定位到的文本片段是正确的。但是有些问题，虽然定位的还算是一个相关的片段，但是的确无法得出答案。
在这个过程中，可以看出运行效果好坏取决于单机的开源小模型能够承载的文本输入的长度问题。我们使用 OpenAI 的 gpt-3.5-turbo 模型的时候，4096 个 Token，也就是一个文本片段可以放上上千字在里面。

但是我们这里单机用的 paraphrase-multilingual-mpnet-base-v2 模型，只能支持 128 个 Token 的输入，虽然对应的 Tokenizer 不一样，但是就算一个字一个 Token，也就 100 个字而已。导致我们检索的内容上下文太少了，没有足够的信息，让语言模型去回答。

这个可以通过把更大的模型部署到云端来解决。

还有一个更难解决的问题，就是模型的推理能力问题。我们可以再试试[01 节](01.md)商品总结英文名称和卖点的例子。

```
question = """Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具

1. Compose human readale product title used on Amazon in english within 20 words.
2. Write 5 selling points for the products in Amazon.
3. Evaluate a price range for this product in U.S.

Output the result in json format with three properties called title, selling_points and price_range"""
response, history = model.chat(tokenizer, question, history=[])
print(response)
```

输出结果：

```
1. title: 充气玩具青蛙夜市地摊卖
2. selling_points:
    - 工厂现货：保证产品质量
    - PVC充气：环保耐用
    - 夜市地摊：方便销售
    - 热卖：最受欢迎产品
    - 儿童水上玩具：适合各种年龄段儿童
3. price_range: (in USD)
    - low:   $1.99
    - high:   $5.99
```

可以看到，虽然这个结果不算太离谱，多少和问题还是有些关系的。但是无论是翻译成英文，还是使用 JSON 返回，模型都没有做到。给到的卖点也没有任何“推理出来”的性质，都是简单地对标题的重复描述。即使你部署一个更大版本的模型到云端，也好不到哪里去。
