#### 从文本 ChatBot 起步

我们先从最简单的文本 ChatBot 起步，先来做一个和第 6 讲一样的文本聊天机器人。对应的代码逻辑和第 6 讲的 ChatGPT 应用基本一样，整个的 UI 界面也还是使用 Gradio 来创建。

唯一的区别在于，我们把原先自己封装的 Conversation 类换成了 Langchain 的 ConversationChain 来实现，并且使用了 SummaryBufferMemory。这样，我们就不需要强行设定只保留过去几轮对话了。

```
import openai, os
import gradio as gr
from langchain import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI

openai.api_key = os.environ["OPENAI_API_KEY"]

memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)
conversation = ConversationChain(
    llm=OpenAI(max_tokens=2048, temperature=0.5),
    memory=memory,
)

def predict(input, history=[]):
    history.append(input)
    response = conversation.predict(input=input)
    history.append(response)
    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]
    return responses, history

with gr.Blocks(css="#chatbot{height:800px} .overflow-y-auto{height:800px}") as demo:
    chatbot = gr.Chatbot(elem_id="chatbot")
    state = gr.State([])

    with gr.Row():
        txt = gr.Textbox(show_label=False, placeholder="Enter text and press enter").style(container=False)

    txt.submit(predict, [txt, state], [chatbot, state])

demo.launch()
```

#### 增加语音输入功能

Gradio 自带 Audio 模块

1. 首先，我们在 Gradio 的界面代码里面增加一个 Audio 组件。这个组件可以录制你的麦克风的声音。
2. 然后，我们封装了一个 transcribe 方法，通过调用 OpenAI 的 Whisper API 就能够完成语音识别。
3. 接着，我们就要把麦克风录好的声音自动发送给语音识别，然后再提交给原先基于文本聊天的机器人就好了。

我们先在 Audio 的 change 事件里，定义了触发 process_audio 的函数。这样，一旦麦克风的声音录制下来，就会直接触发聊天对话，不需要再单独手工提交一次内容。

然后在 process_audio 函数里，我们先是转录对应的文本，再调用文本聊天机器人的 predict 函数，触发对话。

#### 增加语音回复功能

使用 Azure 的语音合成功能就能实现

1. 封装一个函数进行语音合成与播放。
2. 在拿到 ChatGPT 的返回结果之后调用一下这个函数。

#### 用 D-ID 给语音对口型

[D-ID](https://www.d-id.com/) 提供的 API

#### 通过 D-ID 生成视频

#### 将视频嵌入到 Gradio 应用中

1. 我们在原有的 Gradio 界面中，又增加了一个 HTML 组件，显示头像图片，并用来播放对好口型的视频。默认一开始，显示的是一张图片。
2. 在录音转录后触发 Predict 函数的时候，我们不再通过 Azure 的语音合成技术来生成语音，而是直接使用 D-ID 的 API 来生成基于头像的且口型同步的视频动画。并且视频动画在生成之后，将前面 HTML 组件的内容替换成新生成的视频，并自动播放。
3. 在获取视频的时候需要注意一点，就是我们需要等待视频在 D-ID 的服务器生成完毕，才能拿到对应的 result_url。其实更合理的做法是注册一个 webhook，等待 d-id 通过 webhook 通知我们视频生成完毕了，再播放视频。

#### 体验 PaddleGAN 开源模型下的数字主播

不过，使用 D-ID 的价格也不便宜，而前面的各个模块，我其实都给你看过对应的开源解决方案。比如 ChatGPT 我们可以用 ChatGLM 来代替，语音识别我们可以使用本地的 Whisper 模型，语音合成也可以通过 PaddleSpeech 里的 fastspeech2 的开源模型来完成。那么，我们这里也来尝试一下通过开源模型来合成这样的口播视频。

百度 PaddlePaddle 下的 [PaddleBobo](https://github.com/JiehangXie/PaddleBoBo) 开源项目。
