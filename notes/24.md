文生图（Text-To-Image）主流应用：OpenAI 陆续发表了 DALL-E 和 DALL-E 2，Google 发表 Imagen，而市场上实际被用得最多、反馈最好的用户端产品是 Midjourney。社区最流行的是 Stable Diffusion

#### 使用 Stable Diffusion 生成图片

#### 文生图

安装依赖包：

```
%pip install diffusers accelerate transformers
```

代码：

```
from diffusers import DiffusionPipeline
pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipeline.to("cuda")
image = pipeline("a photograph of an astronaut riding a horse").images[0]
image
```

输出结果：

#### Stable Diffusion 的基本原理

Stable Diffusion 背后是由多个模型组合而成的。
整个 Stable Diffusion 文生图的过程是由这样三个核心模块组成的。

1. 第一个模块是一个 Text-Encoder，把我们输入的文本变成一个向量。
2. 第二个是 Generation 模块，顾名思义是一个图片信息生成模块。
3. 最后一个模块，则是 Decoder 或者叫做解码器。

结合 Stable Diffusion 相关论文里的一张模型框架图来看：

我们先把 DiffusionPipeline 打印出来，看看它内部是由哪些部分组成的。

```
pipeline
```

输出结果：

```
StableDiffusionPipeline {
    "_class_name": "StableDiffusionPipeline",
    "_diffusers_version": "0.15.1",
    "feature_extractor": [
        "transformers",
        "CLIPFeatureExtractor"
    ],
    "requires_safety_checker": true,
    "safety_checker": [
        "stable_diffusion",
        "StableDiffusionSafetyChecker"
    ],
    "scheduler": [
        "diffusers",
        "PNDMScheduler"
    ],
    "text_encoder": [
        "transformers",
        "CLIPTextModel"
    ],
    "tokenizer": [
        "transformers",
        "CLIPTokenizer"
    ],
    "unet": [
        "diffusers",
        "UNet2DConditionModel"
    ],
    "vae": [
        "diffusers",
        "AutoencoderKL"
    ]
}
```

1. Tokenizer 和 Text_Encoder，就是我们上面说的把文本变成向量的 Text Encoder。
2. UNet 和 Scheduler，就是对文本向量以及输入的噪声进行噪声去除的组件，也就是 Generation 模块。
3. VAE，也就是解码器（Decoder），这里用的是 AutoencoderKL，它会根据上面生成的图片信息最后还原出一张高分辨率的图片。

剩下的 feature_extractor，可以用来提取图像特征，如果我们不想文生图，想要图生图，它就会被用来把我们输入的图片的特征提取成为向量。而 safety_checker 则是用来检查生成内容，避免生成具有冒犯性的图片。

首先，我们把上面 Stable Diffusion 1.5 需要的模型组件都加载出来。

我们把接下来生成图片的参数初始化一下，包括文本、对应的图片分辨率，以及一系列模型中需要使用的超参数。

然后，我们把对应的输入文本变成一个向量，然后再根据一个空字符串生成一个“无条件”的向量，最后把两个向量拼接在一起。我们实际生成图片的过程，就是逐渐从这个无条件的向量向输入文本表示的向量靠拢的过程。

然后，先生成一系列随机噪声。

接下来就是生成图片的代码了，我们先定义两个函数，它们会分别显示 Generation 模块生成出来的图片信息，以及 Decoder 模块还原出来的最终图片。

最后，我们通过 Diffusion 算法一步一步来生成图片就好了。我们根据前面指定的参数，循环了 25 步，每一步都通过 Scheduler 和 UNet 来进行图片去噪声的操作。并且每 5 步都把对应去噪后的图片信息，以及解码后还原的图片显示出来。

#### 图生图

通过 Stable Diffusion 实现图生图，我们可以直接使用 Diffusers 库里自带的 Pipeline。

```
import torch
from PIL import Image
from io import BytesIO

from diffusers import StableDiffusionImg2ImgPipeline

device = "cuda"
model_id_or_path = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.fl)
pipe = pipe.to(device)

image_file = "./data/sketch-mountains-input.jpg"

init_image = Image.open(image_file).convert("RGB")
init_image = init_image.resize((768, 512))

prompt = "A fantasy landscape, trending on artstation"

images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images

display(init_image)
display(images[0])
```

对应的代码也非常简单，我们把 Pipeline 换成了 StableDiffusionImg2ImgPipeline，此外除了输入一段文本之外，我们还提供了一张草稿图。

StableDiffusionImg2ImgPipeline 的生成过程，其实和我们之前拆解的一步步生成图片的过程是相同的。<b>唯一的一个区别是，我们其实不是从一个完全随机的噪声开始的，而是把对应的草稿图，通过 VAE 的编码器，变成图像生成信息，又在上面加了随机的噪声。</b>所以，去除噪音的过程中，对应的草稿图的轮廓就会逐步出现了。而在一步步生成图片的过程中，内容又会向我们给出的提示语的内容来学习。

#### 更多使用方法

理解了 Stable Diffusion 的基本框架，你可以试一试更多相关的 Pipeline 的用法。比如，除了引导内容生成的提示语，我们还可以设置一个负面的提示语（negative prompt），也就是排除一些内容。

```
prompt = "ghibli style, a fantasy landscape with castles"
negative_prompt = "river"
images = pipe(prompt=prompt, negative_prompt=negative_prompt, image=init_image, strength=0.7)
display(images[0])
```

#### 使用社区里的其他模型
