语音合成（Text-To-Speech）
国内外的各大公司都有类似的云服务，比如科大讯飞、阿里云、百度、AWS Polly、Google Cloud 等等。

#### 使用 Azure 云进行语音合成

1. 因为微软和 OpenAI 有合作，Azure 还提供了 OpenAI 相关模型的托管。这样，我们在实际的生产环境使用的时候，只需要和一个云打交道就好了。
2. 价格比较便宜，并且提供了免费的额度。如果你每个月的用量在 50 万个字符以内，那么就不用花钱。

#### 基本的语音合成

```
import os
import azure.cognitiveservices.speech as speechsdk

# This example requires environment variables named "SPEECH_KEY" and "SPEECH_REGION"
speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('AZURE_SPEECH_KEY'), region=os.environ.get('AZURE_SPEECH_REGION'))
audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)

# The language of the voice that speaks.
speech_config.speech_synthesis_voice_name='zh-CN-XiaohanNeural'

speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)

text = "今天天气真不错，ChatGPT真好用。"

speech_synthesizer.speak_text_async(text)
```

1. 我们先通过配置读取了 API Key 和 Region。
2. 然后通过 speech_synthesis_voice_name 这个配置参数指定了我们合成语音所使用的声音。
3. 通过 speak_text_async 这个函数，就能异步调用 API 服务，直接把合成的声音播放出来了。

通过 speech_synthesis_voice_name 这个参数，我们还可以选用很多别的声音，包括不同语言和不同的人。对应的列表可以在 Azure 的 [Language and voice support](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=tts#prebuilt-neural-voices) 文档里面找到。我们换一个其他的 voice_name，就可以把对应的语音换成男声。

```
speech_config.speech_synthesis_voice_name='zh-CN-YunfengNeural'
speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)
speech_synthesizer.speak_text_async(text)
```

#### 指定语音的风格与角色

在很多 voice_name 里，我们还有额外的两个参数可以选择，那就是 Styles 和 Roles，它们分别代表了合成语音的语气和对应的角色。通过这两个参数，我们可以让 AI 把很多场景“演出来”。

Azure 并不是通过让你在 API 里面配置一些参数来指定一段文本的角色和语气，而是通过一个叫做 SSML 格式的 XML 文件做到这一点的。这个 SSML 是 Speech Synthesis Markup Language 的首字母缩写，翻译过来就是语音合成标记语言。并不是 Azure 专属，而是一个 W3C 标准。

#### 指定语音的输出方式

我们只需要把原先设置成 use_default_speaker=True 的 AudioOutputConfig，改为设置成一个 .wav 的输出文件就好了。我们之后调用 speak_text_async 的函数，就会把语音输出到相应的.wav 文件里。

#### 使用开源模型进行语音合成

很多时候因为数据安全问题，我们希望部署在自己的服务器上进行语音合成。百度开源的 PaddleSpeech 的语音合成功能。

首先安装依赖包：

```
%pip install paddlepaddle
%pip install paddlespeech
```

然后通过 PaddleSpeech 自带的 TTSExecutor，可以将对应的文本内容转换成 WAV 文件。

```
from paddlespeech.cli.tts.infer import TTSExecutor

tts_executor = TTSExecutor()

text = "今天天气十分不错，百度也能做语音合成。"
output_file = "./data/paddlespeech.wav"
tts_executor(text=text, output=output_file)
```

如果需要在 Python 里面播放对应的声音，我们还要借助于 PyAudio 这个包。

```
brew install portaudio // brew
sudo apt-get install portaudio19-dev // Unbuntu

// 只有在 portaudio 安装成功之后，我们才能安装 PyAudio 包
pip install pyaudio
```

```
import wave
import pyaudio

def play_wav_audio(wav_file):
    # open the wave file
    wf = wave.open(wav_file, 'rb')

    # instantiate PyAudio
    p = pyaudio.PyAudio()

    # open a stream
    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                    channels=wf.getnchannels(),
                    rate=wf.getframerate(),
                    output=True)

    # read data from the wave file and play it
    data = wf.readframes(1024)
    while data:
        // 写入声音
        stream.write(data)
        data = wf.readframes(1024)

    # close the stream and terminate PyAudio
    stream.stop_stream()
    stream.close()
    p.terminate()

play_wav_audio(output_file)
```

PaddleSpeech 默认情况下使用的是一个只支持中文的模型。我们可以通过一些参数来指定使用的模型，一样能够做中英文混合的语音合成。

```
tts_executor = TTSExecutor()

text = "早上好, how are you? 百度Paddle Speech一样能做中英文混合的语音合成。"
output_file = "./data/paddlespeech_mix.wav"
tts_executor(text=text, output=output_file,
             am="fastspeech2_mix", voc="hifigan_csmsc",
             lang="mix", spk_id=174)

play_wav_audio(output_file)
```

和上面的代码相比，我们增加了 4 个参数：

1. am，是 acoustic model 的缩写，也就是我们使用的声学模型。我们这里选用的是 fastspeech2_mix。fastspeech2 也是一个基于 Transformer 的语音合成模型，速度快、质量高。这里带了一个 mix，代表这个模型是支持中英文混合生成的。
2. voc，是 vocoder 的缩写，叫做音码器。声学模型只是把我们的文本变成了一个声音波形的信号。我们这里选择的 HiFiGAN_csMSC，是一个高保真（HiFi）、基于对抗生成网络（GAN）技术的模型，它的训练数据用到了 HiFiSinger 和 csMSC，而模型的名字就来自这些关键词的组合。
3. lang，代表我们模型支持的语言，这里我们自然应该选 mix。
4. spk_id，类似于我们之前在 Azure 里看到的 voice_name，不同的 spk_id 听起来就是不同的人说的话。
