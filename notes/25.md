Stable Diffusion 对画画的控制不是很稳定，即使是 img2img，提供一张图开对图片产生一定的控制，随机性也很强。

ControlNet 是在 Stable Diffusion 的基础上进行优化的一个开源项目，它既对原本的模型架构进行了修改，又在此基础上进行了进一步地训练，提供了一系列新的模型供你使用。

#### ControlNet 模型

安装依赖包

```
%pip install diffusers transformers xformers accelerate
%pip install opencv-contrib-python
%pip install controlnet_aux
```

#### 通过边缘检测绘制头像

我们先要通过 OpenCV 对图片做一下预处理。我们先定义了一个 get_canny_image 的函数，这个函数可以根据我们设置的 low_threshold 和 high_threshold 对图片进行边缘检测。低于 low_threshold 的部分会被忽略，高于 high_threshold 的部分会被认为是边缘，而在两者之间的则会根据和其他边缘的连接情况来判定。检测完之后，边缘处的像素值是 255（白色），非边缘处的则是 0（黑色）。

对原始图片调用 get_canny_image，再将原始图片和边缘检测之后的图片通过 display_images 分列左右展示出列

在有了边缘检测的底图之后，我们就可以使用 ControlNet 的模型来画图了。

首先，我们还是通过 Diffusers 库的 Pipeline 功能来加载模型。(Stable Diffusion 1.5, controlnet-canny )

在模型加载完成之后，我们还对 Pipeline 设置了两个配置：

1. enable_cpu_offload 会在 GPU 显存不够用的时候，把不需要使用的模型从 GPU 显存里移除，放到内存里面。
2. enable_xformers_memory_efficient_attention 则是通过我们安装好的 xformers 库来加速模型推理。

在 Pipeline 加载完成之后，一次性画了 4 张图片。使用 Diffusers 的 Pipeline 可以对数据进行批处理，4 段 Prompt 是一起被 CLIP 模型处理成向量的，对应的 4 张图片也是同时一步步生成的。

#### 通过“动态捕捉”来画人物图片

使用 ControlNet 通过 OpenposeDetector 可以精准的捕捉到两个雕塑的姿势。

#### 通过简笔画来画出好看的图片

还有一种常见的 ControlNet 模型叫做 Scribble，它的效果就是能够让你以一个简单的简笔画
为基础，生成精美的图片。

#### ControlNet 支持的模型

ControlNet 一共训练了 8 个不同的模型，除了上面 3 个之外，还包括以下 5 种。

1. HED Boundary，这是除 Canny 之外，另外一种边缘检测算法获得的边缘检测图片。我测试效果往往还比 Canny 更好一些。
2. Depth，深度估计，也就是对一张图片的前后深度估计出来的轮廓图。
3. Normal Map，法线贴图，通常在游戏中用得比较多，可以在不增加模型复杂性的情况下，提升细节效果。
4. Semantic Segmentation，语义分割图，可以把图片划分成不同的区域模块。上一讲里我们拿来生成宫崎骏风格的城堡的底图，风格就类似于一个语义分割图。
5. M-LSD，这个能够获取图片中的直线段，很适合用来给建筑物或者房间内的布局描绘轮 廓。这个算法也常常被用在自动驾驶里面。
