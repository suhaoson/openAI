OpenClip: 多模态的 CLIP 模型

CLIP 的思路，互联网上已有的大量公开的图片数据。而且其中有很多已经通过 HTML 标签里面的 title 或者 alt 字段，提供了对图片的文本描述。

训练一个模型，将文本转换成一个向量，也将图片转换成一个向量。图片向量应该和自己的文本描述向量的距离尽量近，和其他的文本向量要尽量远。那么这个模型，就能够把图片和文本映射到同一个空间里。

#### 图片的零样本分类

刚刚介绍过的 Transformers 可以说是当今大模型领域事实上的标准，那我就还是用 Transformers 库来给举个例子，代码如下：

```
import torch
from PIL import Image
from IPython.display import display
from IPython.display import Image as IPyImage
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def get_image_feature(filename: str):
    image = Image.open(filename).convert("RGB")
    processed = processor(images=image, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        image_features = model.get_image_features(pixel_values=processed["pixel_values"])
    return image_features

def get_text_feature(text: str):
    processed = processor(text=text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        text_features = model.get_text_features(processed['input_ids'])
    return text_features

def cosine_similarity(tensor1, tensor2):
    tensor1_normalized = tensor1 / tensor1.norm(dim=-1, keepdim=True)
    tensor2_normalized = tensor2 / tensor2.norm(dim=-1, keepdim=True)
    return (tensor1_normalized * tensor2_normalized).sum(dim=-1)

image_tensor = get_image_feature("./data/cat.jpg")

cat_text = "This is a cat."
cat_text_tensor = get_text_feature(cat_text)

dog_text = "This is a dog."

dog_text_tensor = get_text_feature(dog_text)

display(IPyImage(filename='./data/cat.jpg'))

print("Similarity with cat : ", cosine_similarity(image_tensor, cat_text_tensor))
print("Similarity with dog : ", cosine_similarity(image_tensor, dog_text_tensor))
```

输出结果：
![]()

1. 我们先是通过 Transformers 库的 CLIPModel 和 CLIPProcessor，加载了 clip-vit-base-patch32 这个模型，用来处理我们的图片和文本信息。
2. 在 get_image_features 方法里，我们做了两件事情。
   首先，我们通过刚才拿到的 CLIPProcessor 对图片做预处理，变成一系列的数值特征表示的向量。这个预处理的过程，其实就是把原始的图片，变成一个个像素的 RGB 值；然后统一图片的尺寸，以及对于不规则的图片截取中间正方形的部分，最后做一下数值的归一化。具体的操作步骤，已经封装在 CLIPProcessor 里了，你可以不用关心。
   然后，我们再通过 CLIPModel，把上面的数值向量，推断成一个表达了图片含义的张量（Tensor）。这里，你就把它当成是一个向量就好了。
3. 同样的，get_text_feature 也是类似的，先把对应的文本通过 CLIPProcessor 转换成 Token，然后再通过模型推断出表示文本的张量。
4. 然后，我们定义了一个 cosine_similarity 函数，用来计算两个张量之间的余弦相似度。
5. 最后，我们就可以利用上面的这些函数，来计算图片和文本之间的相似度了。我们拿了一张程序员们最喜欢的猫咪照片，和“This is a cat.” 以及 “This is a dog.” 的文本做比较。可以看到，结果的确是猫咪照片和“This is a cat.” 的相似度要更高一些。

我们可以再多拿一些文本来进行比较。图片里面，实际是 2 只猫咪在沙发上，那么我们分别试试"There are two cats."、"This is a couch."以及一个完全不相关的“This is a truck.”，看看效果怎么样。

```
two_cats_text = "There are two cats."
two_cats_text_tensor = get_text_feature(two_cats_text)
truck_text = "This is a truck."
truck_text_tensor = get_text_feature(truck_text)
couch_text = "This is a couch."
couch_text_tensor = get_text_feature(couch_text)
print("Similarity with cat : ", cosine_similarity(image_tensor, cat_text_tensor))
print("Similarity with dog : ", cosine_similarity(image_tensor, dog_text_tensor))
print("Similarity with two cats : ", cosine_similarity(image_tensor, two_cats_text_tensor))
print("Similarity with truck : ", cosine_similarity(image_tensor, truck_text_tensor))
print("Similarity with couch : ", cosine_similarity(image_tensor, couch_text_tensor))
```

输出结果：

```
Similarity with cat : tensor([0.2482])
Similarity with dog : tensor([0.2080])
Similarity with two cats : tensor([0.2723])
Similarity with truck : tensor([0.1814])
Similarity with couch : tensor([0.2376])
```

由上可以看出，"There are two cats." 相似度最高，不相关的 "This is a truck."相似度最低。

#### 通过 CLIP 进行目标检测

零样本下的目标检测，目标检测其实就是是在图像中框出特定区域，然后对这个区域内的图像内容进行分类。因此，我们同样可以用 CLIP 来实现目标检测任务。

Google 就基于 CLIP，开发了 OWL-ViT 这个模型来做零样本的目标检测，我们可以直接使用上一讲学过的 Pipeline 来试一试它是怎么帮助我们做目标检测的。

目标检测：

```
from transformers import pipeline
detector = pipeline(model="google/owlvit-base-patch32", task="zero-shot-object-detection")
detected = detector(
"./data/cat.jpg",
candidate_labels=["cat", "dog", "truck", "couch", "remote"],
)
print(detected)
```

输出结果：

```
[{'score': 0.2868116796016693, 'label': 'cat', 'box': {'xmin': 324, 'ymin': 20, 'xmax': 640 }}]
```

先定义了一下 model 和 task，然后输入了我们用来检测的图片，以及提供的类别就完事了。从打印出来的结果中可以看到，里面包含了模型检测出来的所有物品的边框位置。这一次，我们还特地增加了一个 remote，也就是遥控器的类别，看看这样的小物体模型是不是也能识别出来。
接下来，我们就把边框标注到图片上，看看检测的结果是否准确。
首先，我们需要安装一下 OpenCV。

```
pip install opencv-python
```

后面的代码也很简单，就是遍历一下上面检测拿到的结果，然后通过 OpenCV 把边框绘制到图片上就好了。

```
import cv2
from matplotlib import pyplot as plt

# Read the image
image_path = "./data/cat.jpg"
image = cv2.imread(image_path)

# Convert the image from BGR to RGB format
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Draw the bounding box and label for each detected object
for detection in detected:
    box = detection['box']
    label = detection['label']
    score = detection['score']

    # Draw the bounding box and label on the image
    xmin, ymin, xmax, ymax = box['xmin'], box['ymin'], box['xmax'], box['ymax']
    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
    cv2.putText(image, f"{label}: {score:.2f}", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX,)

# Display the image in Jupyter Notebook
plt.imshow(image)
plt.axis('off')
plt.show()
```

#### 商品搜索与以图搜图
