<b>ChatCompletion</b>，对应的模型叫做 gpt-3.5-turbo
官方文档示例：

```
import openai
openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020"}
        {"role": "user", "content": "Where was it played?"}
    ]
)
```

在 OpenAI 的官方文档里，可以看到这个接口也非常简单。你需要传入的参数，从一段 Prompt 变成了一个数组，数组的每个元素都有 role 和 content 两个字段。

1. role 这个字段一共有三个角色可以选择，其中 system 代表系统，user 代表用户，而 assistant 则代表 AI 的回答。
2. 当 role 是 system 的时候，content 里面的内容代表我们给 AI 的一个指令，也就是告诉 AI 应该怎么回答用户的问题。
3. 而当 role 是 user 或者 assistant 的时候，content 里面的内容就代表用户和 AI 对话的内容。和我们在[第 03 讲](./03.md)里做的聊天机器人一样，你需要把历史上的对话一起发送给 OpenAI 的接口，它才能有理解整个对话的上下文的能力。

封装一个智能聊天机器人：

```
import openai
import os

openai.api_key = os.environ.get("#OPENAI_API_KEY")

class Conversation:
    def __init__(self, prompt, num_of_round):
        self.prompt = prompt
        self.num_of_round = num_of_round
        self.messages = []
        self.messages.append({"role": "system", "content": self.prompt})

    def ask(self, question):
        try:
            self.messages.append({"role": "user", "content": question})
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=self.messages,
                temperature=0.5,
                max_tokens=2048,
                top_p=1,
            )
        except Exception as e:
            print(e)
            return e

    message = response["choices"][0]["message"]["content"]
    self.messages.append({"role": "assistant", "content": message})

    if len(self.messages) > self.num_of_round*2 + 1:
        del self.messages[1:3] //Remove the first round conversation left.
    return message
```

1. 我们封装了一个 Conversation 类，它的构造函数 init 会接受两个参数，prompt 作为 system 的 content，代表我们对这个聊天机器人的指令，num_of_round 代表每次向 ChatGPT 发起请求的时候，保留过去几轮会话。
2. Conversation 类本身只有一个 ask 函数，输入是一个 string 类型的 question，返回结果也是 string 类型的一条 message。
3. 每次调用 ask 函数，都会向 ChatGPT 发起一个请求。在这个请求里，我们都会把最新的问题拼接到整个对话数组的最后，而在得到 ChatGPT 的回答之后也会把回答拼接上去。如果回答完之后，发现会话的轮数超过我们设置的 num_of_round，我们就去掉最前面的一轮会话。

#### 计算聊天机器人的成本

在[第 03 讲](./03.md)，我们要发送大量的聊天记录给到 OpenAI，这是由于 OpenAI 的 GPT-3 大语言模型的原理所决定的。GPT-3 能够实现的功能非常简单，根据一段文字生成后续的内容。而为了能够方便所有人使用，OpenAI 的整个回话过程需要自己去拼接。

即使 ChatGPT 的接口是把对话分成了一个数组，但是实际上，最终发送给模型的还是拼接到一起的字符串。ChatGPT 在他的 python 库提供了一种 ChatML 格式，OpenAI 实际做的，就是根据一个定义好特定分隔符的格式，将你提供的多轮对话的内容拼接在一起，提交给 gpt-3.5-turbo 这个模型。

```
<|im_start|>system
You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.
Knowledge cutoff: 2021-09-01
Current date: 2023-03-01<|im_end|>
<|im_start|>user
How are you<|im_end|>
<|im_start|>assistant
I am doing well!<|im_end|>
<|im_start|>user
How are you now?<|im_end|>
```

注：chatml 的文档里，你可以看到你的对话，就是通过 <|im_start|>system|user|assistant、<|im_end|> 这些分隔符分割拼装的字符串。底层仍然是一个内容续写的大语言模型。

OpenAI 是通过模型处理的 Token 数量来收费的，但是要注意，这个收费是“双向收费”。

#### 通过 API 计算 Token 数量

```
code...
num_of_tokens = response['usage']['total_tokens']
code...

if len(self.messages) > self.num_of_round*2 + 1:
del self.messages[1:3]
return message, num_of_tokens
```

#### 通过 Tiktoken 库计算 Token 数量

第二种方式，我们在上一讲用过，就是使用 Tiktoken 这个 Python 库，将文本分词，然后数一
数 Token 的数量。

需要注意，使用不同的 GPT 模型，对应着不同的 Tiktoken 的编码器模型。对应的文档，可以查询这个链接：https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb

#### Gradio 帮你快速搭建一个聊天界面

我们直接选用 Gradio 这个 Python 库来开发这个聊天机器人的界面，因为它有这样几个好处。

1. 我们现有的代码都是用 Python 实现的，你不需要再去学习 JavaScript、TypeScript 以及相关的前端框架了。
2. Gradio 渲染出来的界面可以直接在 Jupyter Notebook 里面显示出来，对于不了解技术的同学，也不再需要解决其他环境搭建的问题。
3. Gradio 这个公司，已经被目前最大的开源机器学习模型社区 HuggingFace 收购了。你可以免费把 Gradio 的应用部署到 HuggingFace 上。
4. 用 HuggingFace+Gradio 的部署方式，特别方便我们演示给其他人看。

注：Gradio 官方也有用其他开源预训练模型创建 Chatbot 的教程
[](https://gradio.app/creating-a-chatbot/)

```
conda install -c conda-forge gradio
```

Gradio 逻辑也很简单：

1. 首先，我们定义好了 system 这个系统角色的提示语，创建了一个 Conversation 对象。
2. 然后，我们定义了一个 answer 方法，简单封装了一下 Conversation 的 ask 方法。主要是通过 history 维护了整个会话的历史记录。并且通过 responses，将用户和 AI 的对话分组。然后将它们两个作为函数的返回值。这个函数的签名是为了符合 Gradio 里 Chatbot 组件的函数签名的需求。
3. 最后，我们通过一段 with 代码，创建了对应的聊天界面。Gradio 提供了一个现成的 Chatbot 组件，我们只需要调用它，然后提供一个文本输入框就好了。

#### 把机器人部署到 HuggingFace 上去

1. 首先注册一个 HuggingFace 的账号
2. 在接下来的界面里，给你的 Space 取一个名字，然后在 Select the Space SDK 里面，选择第二个 Gradio。硬件选择免费的，项目选择 public
3. 创建成功后，会跳转到 HuggingFace 的 App 界面。接下来只要通过 git 把当前 space 下载下来，提交两个文件就可以了，分别是
   app.py 包含了我们的 Gradio 应用；
   requirements.txt 包含了这个应用依赖的 Python 包，这里我们只依赖 OpenAI 这一个包。
4. 因为我们的代码里是通过环境变量获取 OpenAI 的 API Key 的，所以我们还要在这个 HuggingFace 的 Space 里设置一下这个环境变量。
   Settings-> Repository secret ->
   在 Name 这里输入 OPENAI_API_KEY，然后在 Secret value 里面填入你的 OpenAI 的密钥。->
   设置完成之后，你还需要点击一下 Restart this space 确保这个应用重新加载一遍，以获取到新设置的环境变量。
